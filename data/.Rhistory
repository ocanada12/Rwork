library(rpart.plot) # 분류트리 시각화
setwd("c:/Rwork/data")
weather = read.csv("weather.csv", header=TRUE)
# 단계2 : 데이터 샘플링
weather.df <- weather[, c(-1,-14)]
idx <- sample(1:nrow(weather.df), nrow(weather.df)*0.7)
weather_train <- weather.df[idx, ]
weather_test <- weather.df[-idx, ]
# 단계2 : 데이터 샘플링
weather.df <- weather[, c(-1,-14)]
idx <- sample(1:nrow(weather.df), nrow(weather.df)*0.7)
weather_train <- weather.df[idx, ]
weather_test <- weather.df[-idx, ]
# 단계1 : 데이터 가져오기
library(rpart) # model 생성
library(rpart.plot) # 분류트리 시각화
setwd("c:/Rwork/data")
weather = read.csv("weather.csv", header=TRUE)
# 단계2 : 데이터 샘플링
weather.df <- weather[, c(-1,-14)]
idx <- sample(1:nrow(weather.df), nrow(weather.df)*0.7)
weather_train <- weather.df[idx, ]
weather_test <- weather.df[-idx, ]
weather_train
# 단계3 : 분류모델 생성
rpart(RainTomorrow ~ .,data = weather_train)
# 단계3 : 분류모델 생성
model <- rpart(RainTomorrow ~ .,data = weather_train)
model
# 단계4 : 분류모델 시각화 - 중요변수 확인
rpart.plot(model)
weather_train <- weather.df[idx, ] # 모델 학습
# 단계5 : 예측 확률 범주화('Yes Rain', 'No Rain')
y_pred <- predict(model,test)
# 단계5 : 예측 확률 범주화('Yes Rain', 'No Rain')
y_pred <- predict(model,weather_test)
y_pred
y_true <- weather_test$RainTomorrow
y_true
y_pred
y_true
weather_test
str
str(y_pred)
y_pred1 <- data.frame(y_pred)
y_pred1
str(y_pred1)
a <- ifelse(y_pred1$Yes>=0.5,"Yes Rain","No Rain")
a
table(y_true,a)
y_pred2 <- ifelse(y_pred1$Yes>=0.5,"Yes Rain","No Rain")
table(y_true,y_pred2)
acc <- (87+7)/nrow(test)
# 단계6 : 혼돈 matrix 생성 및 분류 정확도 구하기
table(y_true,y_pred2)
acc <- (87+7)/nrow(test)
acc
nrow(test)
acc <- (87+7)/nrow(weather_test)
acc
titanic3 <- read.csv(titanic3.csv)
##########################
# Titanic 분류분석
##########################
setwd("c:/Rwork/data")
titanic3 <- read.csv(titanic3.csv)
titanic3 <- read.csv("titanic3.csv")
str(titanic3)
# int -> Factor(범주형)
titanic3$survived <- factor(titanic3$survived,levels = c(0,1))
titanic3$survived
str(titanic3)
table(titanic3$survived)
809 / 1309 # 0.618029
# subset 생성 : 불필요한 칼럼 제외
titanic <- titanic3[-c(3,8,10,12:14)]
str(titanic)
# train/test set
idx <- sample(nrow(titanic),nrow(titanic)*0.8)
train <- titanic[idx,]
test <- titanic[-idx,]
model <- rpart(survived ~.,data = train)
rpart.plot(model)
model <- rpart(survived ~.,data = train)
rpart.plot(model)
y_pred <- predict(model,test,type="class")
y_pred
y_true <- test$survived
table(y_true,y_pred)
acc <- (165+57)/nrow(test)
acc
table(test$survived)
table(y_true,y_pred)
# 재현율  yes yes
recall <- (57) / (29+57)
# 정확률 ues ues
precision <- 57 / (11+57)
# F1 score
f1_score = 2*((precision * recall )/precision + recall)
f1_score
# F1 score
f1_score = 2*((precision * recall )/(precision + recall)
# F1 score
f1_score
# F1 score
f1_score = 2*((precision * recall )/(precision + recall))
f1_score
# 패키지 설치
install.packages("randomForest")
library(randomForest)
names(iris)
# 1. model
randomForest(Species ~ ., data = iris)
# 1. model
model <- randomForest(Species ~ ., data = iris)
# 500 dataset 생성 -> 500 tree(model) -> 예측치
model
(50+47+47)/150
ntree = 400, mtry =2,
model2 <- randomForest(Species ~ ., data = iris,
model2 <- randomForest(Species ~ ., data = iris,
ntree = 400, mtry =2,
importance = TRUE,
na.action = na.omit)
model2
model2 <- randomForest(Species ~ ., data = iris,
ntree = 400, mtry =2,
imporance = TRUE,
na.action = na.omit)
model2
importance(model2)
?randomForest
varImpPlot(model2)
##########################
## 회귀트리(y변수 : 연속형)
##########################
library(MASS)
##########################
## 회귀트리(y변수 : 연속형)
##########################
library(MASS)
data("Boston")
str(Boston)
p = 13
(1/3) * p
p = 14
(1/3) * p # 4.33333
mtry = (1/3)*p
mtry = round((1/3)*p)
mtry
mtry = floor((1/3)*p)
mtry
model3 <- randomForest(medv ~ ., data = Boston,
ntree = 500, mtry = mtry,
imporance = TRUE,
na.action = na.omit)
model3
# 중요변수 시각화
varImpPlot(model3)
# 중요변수 시각화
varImpPlot(model3)
setwd("c:/Rwork/data")
weatherAUS = read.csv("weatherAUS.csv")
str(weatherAUS)
weatherAUS1 <- weatherAUS[-c(1,2,22,23)]
weatherAUS1
str(weatherAUS1)
model3 <- randomForest(Species ~ ., data = iris,
ntree = 400, mtry =2,
imporance = TRUE,
na.action = na.omit)
str(weatherAUS1)
model3 <- randomForest(RainTomorrow ~ ., data = weatherAUS1,
ntree = 500, mtry =2,
imporance = TRUE,
na.action = na.omit)
model3
model3
model3 <- randomForest(RainTomorrow ~ ., data = weatherAUS1,
ntree = 500, mtry =2,
imporance = TRUE,
na.action = na.omit)
model3
model3 <- randomForest(RainTomorrow ~ ., data = weatherAUS1,
ntree = 500, mtry =2,
imporance = TRUE,
na.action = na.omit)
model3
names(iris)
# 1. model
model <- randomForest(Species ~ ., data = iris)
model
iris
(12604 + 2304)/nrow(weatherAUS1)
model3
# 02. 변수의 중요도 평가를 통해서 가장 중요한 변수를 확인하고, 시각화 하시오.
varImpPlot(model3)
model3 <- randomForest(RainTomorrow ~ ., data = weatherAUS1,
ntree = 100, mtry =2,
imporance = TRUE,
na.action = na.omit)
model3
# 02. 변수의 중요도 평가를 통해서 가장 중요한 변수를 확인하고, 시각화 하시오.
varImpPlot(model3)
Humudufy3pm
setwd("c:/Rwork/data")
weatherAUS = read.csv("weatherAUS.csv")
str(weatherAUS)
weatherAUS1 <- weatherAUS[-c(1,2,22,23)]
str(weatherAUS1)
model3 <- randomForest(RainTomorrow ~ ., data = weatherAUS1,
ntree = 100, mtry =2,
imporance = TRUE,
na.action = na.omit)
model3
(12604 + 2304)/nrow(weatherAUS1) 0.
# 02. 변수의 중요도 평가를 통해서 가장 중요한 변수를 확인하고, 시각화 하시오.
varImpPlot(model3)
# Humudufy3pm
# Humudufy3pm
# Humudufy3pm
# Humudufy3pm
# Humudufy3pm
# Humudufy3pm
# Humudufy3pm
# 실습파일 가져오기
setwd("c:/Rwork/data")
data <- read.csv("cleanDescriptive.csv")
str(data)
# 변수 선택
x <- data$level2 # 부모의 학력수준
y <- data$pass2 # 자녀의 합격유무
table(x)
# 고졸     대졸 대학원졸
#  93       86       57
table(y)
df <- data.frame(Level = x, Pass = y)
# 1) 교차분할표 작성(교차표)
table(df$Level,df$Pass) # (행, 열)
# 2) package 이용
install.packages("gmodels")
library(gmodels)
library(help="gmodels")
CrossTable(x=df$Level,y=df$Pass)
# 1) 기댓값 = (셀의 행합 * 셀의 열합) / 전체합
p_value = (89*135)/225
p_value # 53.4
# 2) 기대비율 = (관측값 - 기댓값)^2 / 기댓값
p_rate = (49-p_value)^2 / p_value
# 귀무가설 : 부모의 학력수준이 자녀의 대학진학에 어떤 영향을 미치는가?
CrossTable(x = df$Level, y = df$Pass, chisq = TRUE)
# Chi^2 = 2.766951
Chi_2 = 0.544 + 0.363 + 1.026 + 0.684 + 0.091 + 0.060
# d.f. = 2
# 자유도 : 샘플수(n)에서 자유롭게 선택한 수
# n-1
df = (3-1)*(2-1)
chisq.test(c(4,6,17,16,8,9))
# <검정통계량 해석>
# X-squared = 14.2, df = 5
X-squared >= 11.071 # (table) -> 기각
# (2) 선호도 분석
#-----------------------------------------
# 귀무가설 : 기대치와 관찰치는 차이가 없다.
# 대립가설 : 기대치와 관찰치는 차이가 있다.
#-----------------------------------------
data <- textConnection(
"스포츠음료종류  관측도수
1   41
2   30
3   51
4   71
5   61
")
x <- read.table(data, header=T)
x # 스포츠음료종류 관측도수
chisq.test(x$관측도수)
# 독립변수(x)와 종속변수(y) 생성
setwd("c:/Rwork/data")
read.csv("cleanDescriptive.csv")
x <- data$level2 # 부모의 학력수준
y <- data$pass2 # 자녀의 대학진학여부
CrossTable(x, y, chisq = TRUE) #p =  0.2507057
# 1. 파일 가져오기
setwd("C:/SUNMOON/Rwork-I/Part-III")
data <- read.csv("homogenity.csv", header=TRUE)
head(data)
# method와 survery 변수만 서브셋 생성
data <- subset(data, !is.na(survey), c(method, survey))
# 교육방법2 필드 추가
data$method2[data$method==1] <- "방법1"
data$method2[data$method==2] <- "방법2"
data$method2[data$method==3] <- "방법3"
# 만족도2 필드 추가
data$survey2[data$survey==1] <- "1.매우만족"
data$survey2[data$survey==2] <- "2.만족"
data$survey2[data$survey==3] <- "3.보통"
data$survey2[data$survey==4] <- "4.불만족"
data$survey2[data$survey==5] <- "5.매우불만족"
# 3. 교차분할표 작성
table(data$method2, data$survey2)  # 교차표 생성 -> table(행,열)
# 4. 동질성 검정 - 모수 특성치에 대한 추론검정
chisq.test(data$method2, data$survey2)
str(iris)
# x,y 변수 선택
x <- iris$Species
y <- iris$Sepal.Length
chisq.test(x,y)
table(x)
install.packages("dplyr")
library(dplyr)
# dataset %>% function()
iris %>% group_by(Species) %>% summarise(avg=mean(Sepal.Length))
# 1 setosa      5.01
# 1 setosa      5.01
# 2 versicolor  5.94
install.packages("dplyr")
# 1 setosa      5.01
# 2 versicolor  5.94
# 3 virginica   6.59
# 1 setosa      5.01
# 2 versicolor  5.94
# 3 virginica   6.59
# 1 setosa      5.01
# 2 versicolor  5.94
# 3 virginica   6.59
install.packages("dplyr")
# 1 setosa      5.01
# 2 versicolor  5.94
# 3 virginica   6.59
install.packages("dplyr")
library(dplyr)
# dataset %>% function()
iris %>% group_by(Species) %>% summarise(avg=mean(Sepal.Length))
# 1 setosa      5.01
# 1 setosa      5.01
# 2 versicolor  5.94
# 1 setosa      5.01
# 2 versicolor  5.94
# 3 virginica   6.59
# 차트 데이터 생성
chart_data <- c(305, 450, 320, 460, 330, 480, 380, 520)
names(chart_data) <- c("2016 1분기","2017 1분기","2016 2분기","2017 2분기","2016 3분기","2017 3분기","2016 4분기","2017 4분기")
str(chart_data)
chart_data
# 1) 막대차트 시각화
barplot(chart_data, ylim = c(0, 600), col = rainbow(8), main = "2016년도 vs 2017년도 분기별 매출 현황")
?barplot
# 가로막대차트
barplot(chart_data, xlim = c(0, 600), col = rainbow(8), horiz=TRUE, main = "2016년도 vs 2017년도 분기별 매출 현황")
# 1행2열 구조
par(mfrow=c(1,2)) # 1행 2열 그래프 보기
# 개별 막대차트
barplot(VADeaths, beside=T,col=rainbow(5),
main="미국 버지니아주 하위계층 사망비율")
# 누적형 막대차트
barplot(VADeaths, beside=F,col=rainbow(5),
main="미국 버지니아주 하위계층 사망비율")
# 2) 점 차트 시각화
par(mfrow=c(1,1)) # 1행 2열 그래프 보기
dotchart(chart_data, color = c("green","red"),
labels = names(chart_data), xlab = "매출액",
main = "분기별 판매현황")
# 3) 파이 차트 시각화
pie(chart_data, labels = names(chart_data),
col = rainbow(8), cex = 1.2, edges = 4)
title("2016 vs 2017 판매현황")
?pie
# 1) 상자 그래프 시각화
VADeaths
str(VADeaths)
# num [1:5, 1:4] -> matrix 구조
# 행은 다섯개 열은 4개 총 20개 데이터
summary(VADeaths)
boxplot(VADeaths,range=0)
# 2) 히스토그램 시각화 : 대칭성 확인
iris
head(iris)
mean(iris$Sepal.Length)
range(iris$Sepal.Length) # 4.3  7.9
hist(iris$Sepal.Length,xlab = "Sepal.Length",
col = "magenta",
main = "iris 꽃받침 길이",
xlim = c(4.3, 7.9))
range(iris$Sepal.Width) # 2.0  4.4
mean(iris$Sepal.Width)
hist(iris$Sepal.Width,xlab = "Sepal.Width",
col = "green",
main = "iris 꽃받침 넓이",
xlim = c(2.0, 4.4))
# 정규분포 가설 : 정규분포와 차이가 없다.
# 정규성 검정
shapiro.test(iris$Sepal.Length) # p-value = 0.01018 (유의함율) < 알파(유의수준) = 0.05 : 정규성이 아니다. -> 기각
shapiro.test(iris$Sepal.Width) # p-value = 0.1012 (유의함율) > 알파(유의수준) = 0.05 : 정규성이다. -> 채택
# 3) 산점도 시각화
price <- runif(10,  min=1, max = 100)
price
plot(price) # y, x : index
par(mfrow=c(2,2))
plot(price, type="o") # circle
plot(price, type="l") # line
plot(price, type="h") # height
plot(price, type="s") # step
# 만능 차트
par(mfrow=c(1,1))
data()
range(iris$Sepal.Width) # 2.0  4.4
mean(iris$Sepal.Width)
hist(iris$Sepal.Width,xlab = "Sepal.Width",
col = "green",
main = "iris 꽃받침 넓이",
xlim = c(2.0, 4.4))
# 정규분포 가설 : 정규분포와 차이가 없다.
# 정규성 검정
shapiro.test(iris$Sepal.Length) # p-value = 0.01018 (유의함율) < 알파(유의수준) = 0.05 : 정규성이 아니다. -> 기각
shapiro.test(iris$Sepal.Width) # p-value = 0.1012 (유의함율) > 알파(유의수준) = 0.05 : 정규성이다. -> 채택
# 3) 산점도 시각화
price <- runif(10,  min=1, max = 100)
price
plot(price) # y, x : index
par(mfrow=c(2,2))
plot(price, type="o") # circle
plot(price, type="o") # circle
plot(price, type="l") # line
plot(price, type="h") # height
plot(price, type="s") # step
# 만능 차트
par(mfrow=c(1,1))
data()
# 만능 차트
par(mfrow=c(1,1))
data()
# 시계열 데이터 시각화
AirPassengers
plot(AirPassengers)
# 회귀모델 -> 회귀모델 시각화
install.packages("HistData")
library(HistData)
library(help ="HistData")
data(Galton)
Galton
str(Galton)
model <- lm(child ~ parent, data = Galton) #lm(y ~ x)
model # Coefficients:
plot(model)
# 꽃의 종별 변수 비교
unique(iris$Species)
seq(10,5,-2)
x1 <- 1:5  #c(1:5)
x2 <- 2:6
x1;x2
# rbind()
m2 <- rbind(x1,x2)
# cbind()
m3 <- cbind(x1, x2)
m3
z<-x*y
z
# 1) vector vs matrix
x <- 1:3
y <- matrix(1:6, nrow = 2)
dim(y) # 2 3
z<-x*y
z
# apply(x, margin(1/2), fun) : 처리
apply(z, 1, sum) # 20 26
apply(z, 2, mean) # 2.5 6.5 14.0
# 2. 상관분석 시각화
install.packages("corrplot")
library(corrplot)
corrplot(corr, method = "number")
corrplot(corr, method = "square")
corrplot(corr, method = "circle")
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
chart.Correlation(product)
# 3. 공분산
# - 두 변수의 확률분포의 상관관계의 정도를 나타내는 값
cov(product) # 공분산 행렬
#####################
#### IRIS 적용
#####################
cor(iris[-5])
# 양의 상관관계
plot(iris$Petal.Length, iris$Petal.Width)
# 음의 상관관계
plot(iris$Sepal.Width, iris$Petal.Length)
# 3) 회귀모델
model <- lm(income ~ education + women + prestige, data = newdata)
model
# 2. 회귀모델 생성
model <- lm(formula = formula,  data=iris) # 아이리스에 소속된 변수 lm이라는 함수로 회귀 모델 만듦
model
# 5. 모델 생성/평가
formula = Sepal.Length ~ Sepal.Width + Petal.Length
model <- lm(formula = formula,  data=iris)
summary(model) # 모델 평가
names(model)
model
names(model)
model$fitted.values
airquality
data("airquality")
str(airquality)
data(airquality)
str(airquality)
air_df <- airquality[-c(5:6)]
air_df
str(air_df)
model <- lm(Ozone ~ .,data=air_df)
model
summary(model)
